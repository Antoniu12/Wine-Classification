# -*- coding: utf-8 -*-
"""ProiectSI.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10pQoPTkTXb3mwbf1iOWKpkjiKhO4ST6F

# Importuri
"""

import pandas as pd
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, TensorDataset
from sklearn.preprocessing import MinMaxScaler, LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report, recall_score
import matplotlib.pyplot as plt
import seaborn as sns
import joblib
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier

"""# Importarea si vizualizarea datelor

## Importarea datelor
"""

data = pd.read_csv('winequality-red.csv')

"""## Vizualizari"""

data.head()

data.info()

data.describe().T.style.background_gradient(axis=0)

corr = data.corr()
# cmap = sns.diverging_palette(-1, 1, s=100, l=50, n=15, center="dark", as_cmap=True)
plt.figure(figsize=(9, 6))
sns.heatmap(corr, annot=True, fmt='.2f', linewidth=0.5, cmap='Purples', mask=np.triu(corr))
plt.show()

sns.pairplot(data, hue='quality', corner = True, palette='Purples')



"""# Preprocesarea datelor

## Redenumire coloanelor si impartirea datelor
"""

data.rename(columns=lambda x: x.strip().replace(' ', '_'), inplace=True)
data['quality'] = data['quality'].map({8: 'Good', 7: 'Good', 6: 'Middle', 5: 'Middle', 4: 'Bad', 3: 'Bad'})
data.head()

X = data.drop('quality', axis=1) #Feature
y = data['quality'] #Target
le = LabelEncoder()
y_encoded = le.fit_transform(y)

"""Pentru performante mai bune se mapeaza (in ordine alfabetica):

0 -> Bad

1 -> Good

2 -> Medium
"""

le = LabelEncoder()
y_encoded = le.fit_transform(y)

"""## Normalizare"""

scaler = MinMaxScaler()
X_scaled = scaler.fit_transform(X)

"""## Transformarea in tensori"""

X_tensor = torch.tensor(X_scaled, dtype=torch.float32)
y_tensor = torch.tensor(y_encoded, dtype=torch.long)

"""## Impartirea Datasetului

**X_train si y_temp** primesc **80%** din dataset, X_temp si y_temp primesc 20%.

Se imparte datasetul ramas de 20% in **doua parti de cate 10%.**
"""

X_train, X_temp, y_train, y_temp = train_test_split(X_tensor, y_tensor, test_size=0.2, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

"""## Crearea dataloaderului

Daca marim batch size-ul modelul ii mai precis dar dureaza mai mult timp la procesare.
"""

train_dataset = TensorDataset(X_train, y_train)
val_dataset = TensorDataset(X_val, y_val)
test_dataset = TensorDataset(X_test, y_test)

train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)
test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)

"""## Salvarea datelor pentru testare intr-un fisier"""

def save_test_set_to_csv(X_test, y_test, filename='test_set.csv'):
    df = pd.DataFrame(X_test, columns=[f'Feature{j}' for j in range(X_test.shape[1])])
    df['Label'] = y_test

    df.to_csv(filename, index=False)

X_test_np = X_test.numpy()
y_test_np = y_test.numpy()
save_test_set_to_csv(X_test_np, y_test_np)

"""# MLP

## Primul model MLP
"""

class MLPModel(nn.Module):
    def __init__(self, input_dim, output_dim):
        super(MLPModel, self).__init__()
        self.layer1 = nn.Linear(input_dim, 128)
        self.layer2 = nn.Linear(128, 64)
        self.output = nn.Linear(64, output_dim)

    def forward(self, x):
        x = torch.relu(self.layer1(x))
        x = torch.relu(self.layer2(x))
        x = self.output(x)
        return x

"""## Al doilea model MLP"""

class MLPModel2(nn.Module):
    def __init__(self, input_dim, output_dim):
        super(MLPModel2, self).__init__()
        self.layer1 = nn.Linear(input_dim, 64)
        self.output = nn.Linear(64, output_dim)

    def forward(self, x):
        x = torch.relu(self.layer1(x))
        x = self.output(x)
        return x

"""## Al treilea model MLP"""

class MLPModel3(nn.Module):
    def __init__(self, input_dim, output_dim):
        super(MLPModel3, self).__init__()
        self.layer1 = nn.Linear(input_dim, 64)
        self.output = nn.Linear(64, output_dim)

    def forward(self, x):
        x = torch.relu(self.layer1(x))
        x = self.output(x)
        return x

"""## Instantierea modelelor"""

input_dim = X.shape[1]
output_dim = len(np.unique(y_encoded))
model1 = MLPModel(input_dim, output_dim)
model2 = MLPModel2(input_dim, output_dim)
model3 = MLPModel3(input_dim, output_dim)

"""## Functia Loss si optimizatorii

Daca marim learning rate-ul am observat ca se obtin rezultate mai putin stabile.
"""

criterion = nn.CrossEntropyLoss()
optimizer1 = torch.optim.Adam(model1.parameters(), lr=0.005)
optimizer2 = torch.optim.Adam(model2.parameters(), lr=0.005)
optimizer3 = torch.optim.SGD(model3.parameters(), lr=0.01)

"""## Functie de antrenare a modelelor"""

def train_model(model, train_loader, val_loader, criterion, optimizer, epochs, model_name):
    train_losses = []
    val_losses = []
    for epoch in range(epochs):
        model.train()
        train_loss = 0
        for X_batch, y_batch in train_loader:
            optimizer.zero_grad()
            outputs = model(X_batch)
            loss = criterion(outputs, y_batch)
            loss.backward()
            optimizer.step()
            train_loss += loss.item()

        # Validation loss
        val_loss = 0
        model.eval()
        with torch.no_grad():
            for X_batch, y_batch in val_loader:
                outputs = model(X_batch)
                loss = criterion(outputs, y_batch)
                val_loss += loss.item()

        avg_train_loss = train_loss / len(train_loader)
        avg_val_loss = val_loss / len(val_loader)
        train_losses.append(avg_train_loss)
        val_losses.append(avg_val_loss)
        print(f'{model_name} - Epoch {epoch + 1}/{epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}')
    return train_losses, val_losses

"""## Antrenarea si salvarea modelelor"""

train_losses1, val_losses1 = train_model(model1, train_loader, val_loader, criterion, optimizer1, epochs=5, model_name='MLPModel')
train_losses2, val_losses2 = train_model(model2, train_loader, val_loader, criterion, optimizer2, epochs=5, model_name='MLPModel2')
train_losses3, val_losses3 = train_model(model3, train_loader, val_loader, criterion, optimizer3, epochs=5, model_name='MLPModel3')


torch.save(model1.state_dict(), 'mlp_model.pth')
torch.save(model2.state_dict(), 'mlp_model2.pth')
torch.save(model3.state_dict(), 'mlp_model3.pth')

"""## Salvarea ploturilor pentru Validation Loss si Train Loss"""

epochs = range(1, 6)

plt.figure(figsize=(14, 6))

plt.subplot(1, 2, 1)
plt.plot(epochs, train_losses1, label='MLPModel')
plt.plot(epochs, train_losses2, label='MLPModel2')
plt.plot(epochs, train_losses3, label='MLPModel3')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('Training Losses')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(epochs, val_losses1, label='MLPModel')
plt.plot(epochs, val_losses2, label='MLPModel2')
plt.plot(epochs, val_losses3, label='MLPModel3')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('Validation Losses')
plt.legend()

plt.tight_layout()
plt.show()

"""## Functia de validare a modelelor"""

def evaluate_torch_model(model, dataloader, y_true, le, model_name):
    model.eval()
    y_pred = []
    all_probs = []
    with torch.no_grad():
        for X_batch, _ in dataloader:
            outputs = model(X_batch)
            probs = torch.softmax(outputs, dim=1)
            _, predicted = torch.max(outputs, 1)
            y_pred.extend(predicted.cpu().numpy())
            all_probs.extend(probs.cpu().numpy())

    target_names = [str(cls) for cls in le.classes_]
    print(f"{model_name} Classification Report:\n", classification_report(y_true, y_pred, target_names=target_names, zero_division=0))
    cm = confusion_matrix(y_true, y_pred)
    sns.heatmap(cm, annot=True, fmt="d", cmap='Blues', xticklabels=target_names, yticklabels=target_names)
    plt.title(f'{model_name} Confusion Matrix')
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.show()

    recall = recall_score(y_true, y_pred, average='macro')
    uncertainty = np.mean([1 - max(probs) for probs in all_probs])
    print(f'{model_name} Recall: {recall:.4f}')
    print(f'{model_name} Uncertainty: {uncertainty:.4f}')

"""## Valiadrea modelelor"""

print("MLP Model:")
evaluate_torch_model(model1, test_loader, y_test_np, le, "MLPModel")
print("\n")

print("MLP Model 2:")
evaluate_torch_model(model2, test_loader, y_test_np, le, "MLPModel2")
print("\n")

print("MLP Model 3:")
evaluate_torch_model(model3, test_loader, y_test_np, le, "MLPModel3")
print("\n")

"""#Voting pentru modelele mlp

Primul model are o pondere mai mare deoarece a performat mai bine in urma testarilor.
"""

def soft_voting(models, dataloader, y_true, le, model_names, weights):
    all_probs = np.zeros((len(y_true), len(le.classes_)))
    start_idx = 0
    for model, weight in zip(models, weights):
        model.eval()
        batch_probs = []
        with torch.no_grad():
            for X_batch, _ in dataloader:
                outputs = model(X_batch)
                probs = torch.softmax(outputs, dim=1)
                batch_probs.append(probs.cpu().numpy())
        batch_probs = np.vstack(batch_probs)
        all_probs += weight * batch_probs

    y_pred = np.argmax(all_probs, axis=1)
    target_names = [str(cls) for cls in le.classes_]
    print(f"Soft Voting Classification Report:\n", classification_report(y_true, y_pred, target_names=target_names, zero_division=0))
    cm = confusion_matrix(y_true, y_pred)
    sns.heatmap(cm, annot=True, fmt="d", cmap='Blues', xticklabels=target_names, yticklabels=target_names)
    plt.title('Soft Voting Confusion Matrix')
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.show()

    recall = recall_score(y_true, y_pred, average='macro')
    uncertainty = np.mean([1 - max(probs) for probs in all_probs])
    print(f'Soft Voting Recall: {recall:.4f}')
    print(f'Soft Voting Uncertainty: {uncertainty:.4f}')


print("Soft Voting MLP Models:")
models = [model1, model2, model3]
weights = [2, 1, 1]
soft_voting(models, test_loader, y_test_np, le, ["MLPModel", "MLPModel2", "MLPModel3"], weights)
print("\n")

"""# Alte modele

Clasificatorii erau implementati in modelul de unde am luat datasetul. Ei sunt implementati folosind librarii din scikit learn
"""

X_train_np, X_val_test_np, y_train_np, y_val_test_np = train_test_split(X_scaled, y_encoded, test_size=0.2, random_state=42)
X_val_np, X_test_np, y_val_np, y_test_np = train_test_split(X_val_test_np, y_val_test_np, test_size=0.5, random_state=42)

# KNN Classifier
knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(X_train_np, y_train_np)
joblib.dump(knn, 'knn_model.pkl')

# Random Forest Classifier
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train_np, y_train_np)
joblib.dump(rf, 'rf_model.pkl')

"""## Functia de validare a modelelor"""

def evaluate_sklearn_model(model, X_test, y_test, le, model_name):
    y_pred = model.predict(X_test)
    target_names = [str(cls) for cls in le.classes_]
    print(f"{model_name} Classification Report:\n", classification_report(y_test, y_pred, target_names=target_names, zero_division=0))
    cm = confusion_matrix(y_test, y_pred)
    sns.heatmap(cm, annot=True, fmt="d", cmap='Blues', xticklabels=target_names, yticklabels=target_names)
    plt.title(f'{model_name} Confusion Matrix')
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.show()

"""## Validarea modelelor"""

# Evaluate KNN
print("KNN Classifier:")
evaluate_sklearn_model(knn, X_test_np, y_test_np, le, "KNN")
print("\n")

# Evaluate Random Forest
print("Random Forest Classifier:")
evaluate_sklearn_model(rf, X_test_np, y_test_np, le, "Random Forest")
print("\n")